{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyMA2PuFDDyx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Finish the same task as we did on Day 1, instead of using pipeline, let's do it from tokenizer to model, and analyze the vanilla output from scratch."
      ],
      "metadata": {
        "id": "vdqrl7hECRoZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enL-WnAObd2F",
        "outputId": "3d154521-2104-462d-d4f3-389c25357651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
            "\u001b[K     |████████████████████████████████| 365 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.2.2-py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting transformers[sentencepiece]\n",
            "  Downloading transformers-4.22.0-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 50.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 52.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 73.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 64.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 36.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 51.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2022.6.2)\n",
            "Collecting sentencepiece!=0.1.92,>=0.1.91\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 48.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n",
            "Installing collected packages: urllib3, xxhash, tokenizers, responses, multiprocess, huggingface-hub, transformers, sentencepiece, datasets, evaluate\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.4.0 evaluate-0.2.2 huggingface-hub-0.9.1 multiprocess-0.70.13 responses-0.18.0 sentencepiece-0.1.97 tokenizers-0.12.1 transformers-4.22.0 urllib3-1.25.11 xxhash-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gemxGqU9bg_W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "73a37d9f5eaa45078d8998ffec02ad5b",
            "ecdaa47b91a249b780f6b7e9f4d454bd",
            "d2d1fdcbc98e433c9e33d076ffb964a2",
            "94be4aa962fa4b9bacf794c014bc22ac",
            "638ba82b42304603b8fdc4ac7ed06135",
            "2ef7433296794983ad1e3ff91d7b55a5",
            "c2c9a747e29b42ab80d5e8f3a7868805",
            "6a0bf79ea12e44a7af1650d921307223",
            "4ce6c55264c7489e9da21289e8d1898d",
            "c7b6e7ca0d4944bb9c679fdb61ba1561",
            "5b37b1c350b2496b986b5c9294655ff4"
          ]
        },
        "outputId": "9a191a00-6853-4ee8-e405-69ea2b10b79e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving 0 files to the new cache system\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73a37d9f5eaa45078d8998ffec02ad5b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWShgDmLbjrX",
        "outputId": "d56362da-35d1-47b9-fdd9-7a9a53875861"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598049521446228}]"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ],
      "source": [
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xokBmWPYbvLh"
      },
      "outputs": [],
      "source": [
        "Q1_text = \"Eighteen years have gone by, and still I can bring back every detail of that day in the meadow. Washed clean of summer’s dust by days of gentle rain, the mountains wore a deep, brilliant green. The October breeze set white fronds of head-tall grasses swaying. One long streak of cloud hung pasted across a dome of frozen blue. It almost hurt to look at that faroff sky. A puff of wind swept across the meadow and through her hair before it slipped into the woods to rustle branches and send back snatches of distant barking-a hazy sound that seemed to reach us from the doorway to another world. We heard no other sounds. We met no other people. We saw only two bright, red birds leap startled from the center of the meadow and dart into the woods. As we ambled along, Naoko spoke to me of wells. Memory is a funny thing. When I was in the scene, I hardly paid it any mind. I never stopped to think of it as something that would make a lasting impression, certainly never imagined that eighteen years later I would recall it in such detail. I didn’t give a damn about the scenery that day. I was thinking about myself. I was thinking about the beautiful girl walking next to me. I was thinking about the two of us together, and then about myself again. It was the age, that time of life when every sight, every feeling, every thought came back, like a boomerang, to me. And worse, I was in love. Love with complications. Scenery was the last thing on my mind. Now, though, that meadow scene is the first thing that comes back to me. The smell of the grass, the faint chill of the wind, the line of the hills, the barking of a dog: these are the first things, and they come with absolute clarity. I feel as if I can reach out and trace them with a fingertip. And yet, as clear as the scene may be, no one is in it. No one. Naoko is not there, and neither am I. Where could we have disappeared to? How could such a thing have happened? Everything that seemed so important back then-Naoko, and the self I was then, and the world I had then: where could they have all gone? It’s true, I can’t even bring back Naoko’s face-not right away, at least. All I’m left holding is a background, sheer scenery, with no people up front.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMemjfAYcaoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e801f06-ad8e-4a9a-b214-0394530b6192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import tokenize\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmCQv_Tvcijr"
      },
      "outputs": [],
      "source": [
        "Q1_sentence_nltk = tokenize.sent_tokenize(Q1_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zlLi-OTfgMh",
        "outputId": "21455916-6b60-4e14-9bc3-ec4e943319f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Eighteen years have gone by, and still I can bring back every detail of that day in the meadow.',\n",
              " 'Washed clean of summer’s dust by days of gentle rain, the mountains wore a deep, brilliant green.',\n",
              " 'The October breeze set white fronds of head-tall grasses swaying.',\n",
              " 'One long streak of cloud hung pasted across a dome of frozen blue.',\n",
              " 'It almost hurt to look at that faroff sky.',\n",
              " 'A puff of wind swept across the meadow and through her hair before it slipped into the woods to rustle branches and send back snatches of distant barking-a hazy sound that seemed to reach us from the doorway to another world.',\n",
              " 'We heard no other sounds.',\n",
              " 'We met no other people.',\n",
              " 'We saw only two bright, red birds leap startled from the center of the meadow and dart into the woods.',\n",
              " 'As we ambled along, Naoko spoke to me of wells.',\n",
              " 'Memory is a funny thing.',\n",
              " 'When I was in the scene, I hardly paid it any mind.',\n",
              " 'I never stopped to think of it as something that would make a lasting impression, certainly never imagined that eighteen years later I would recall it in such detail.',\n",
              " 'I didn’t give a damn about the scenery that day.',\n",
              " 'I was thinking about myself.',\n",
              " 'I was thinking about the beautiful girl walking next to me.',\n",
              " 'I was thinking about the two of us together, and then about myself again.',\n",
              " 'It was the age, that time of life when every sight, every feeling, every thought came back, like a boomerang, to me.',\n",
              " 'And worse, I was in love.',\n",
              " 'Love with complications.',\n",
              " 'Scenery was the last thing on my mind.',\n",
              " 'Now, though, that meadow scene is the first thing that comes back to me.',\n",
              " 'The smell of the grass, the faint chill of the wind, the line of the hills, the barking of a dog: these are the first things, and they come with absolute clarity.',\n",
              " 'I feel as if I can reach out and trace them with a fingertip.',\n",
              " 'And yet, as clear as the scene may be, no one is in it.',\n",
              " 'No one.',\n",
              " 'Naoko is not there, and neither am I.',\n",
              " 'Where could we have disappeared to?',\n",
              " 'How could such a thing have happened?',\n",
              " 'Everything that seemed so important back then-Naoko, and the self I was then, and the world I had then: where could they have all gone?',\n",
              " 'It’s true, I can’t even bring back Naoko’s face-not right away, at least.',\n",
              " 'All I’m left holding is a background, sheer scenery, with no people up front.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "Q1_sentence_nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "gPW9P-tAkpT5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "2db4d4e9ba354c84ac8405fe73b07944",
            "e33532a1bb92441ca6227518851c776b",
            "48da5fb6a42a40fb908d467425d485f2",
            "0205f9b512be44aaa4a6746b3d94f36b",
            "2e7e1037fcbe4eea841f35ab10c88da6",
            "1d0119e706e84c549b772beb550ea0b0",
            "59b9edb36f184984bda1a72c07ef24fe",
            "85faa4c5157d4af49f4dadc2fe3e0b28",
            "e40f15d7a95f45fb9021bf3ecaa7f3e5",
            "47b57ca5d5e34902877b29a1e5ab499d",
            "fb9c8622477d48d79e40faf995b017b0",
            "9ef4c811594049799ba19cb84706322e",
            "6dd0ce6839f9439987be8e66ad77616d",
            "1fd15415ca6b4833a7cd446ad9d056ce",
            "b251745c74514022a57c718f71cc2217",
            "74060d81beec41a89e90c6f2c37ad2cc",
            "b83ab34d76a84e62be27561bfaaad1ff",
            "e09716607e6d45699a28fa2f338b89e9",
            "2b08f77edb994d81a565fb2746bd42a1",
            "7e22b64abbb34fb09077ebfe0e5ca726",
            "0bcaf59ba20542a9a22dda443e78d0c4",
            "992d20f2211143e7af6b45a0bb3446d9",
            "1a598d56553d4a22a99b9d558f1ea8b2",
            "ef8d83633da8419580d57f1e8c1d3fda",
            "41a7fafeffd64b40971e8bc77ec27e2c",
            "10d236a3c2b840f3bd78f0d718ca9c93",
            "80fb64403feb484b85026d5d8d997cb6",
            "5410c78781d24392b384c2417e2ae1d5",
            "5a19ada24a9b4f49820dfa26e59f2ecd",
            "22af5a20122d4b299baea74a3fbab22d",
            "b0cbfedd286a4fa5828c5cea21993a7d",
            "22fd7461121a47acbc29ebe17a1b6cb0",
            "395f8eb3747e4d4b95af92d5e2882f2a"
          ]
        },
        "outputId": "104690a8-a770-41e0-a2e5-2aa1144946b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2db4d4e9ba354c84ac8405fe73b07944"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ef4c811594049799ba19cb84706322e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a598d56553d4a22a99b9d558f1ea8b2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(Q1_sentence_nltk, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNTZD5hNkPRu",
        "outputId": "a842fa7f-cf9c-41b0-a46e-2d870f4c8341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[ 101, 7763, 2086,  ...,    0,    0,    0],\n",
            "        [ 101, 8871, 4550,  ...,    0,    0,    0],\n",
            "        [ 101, 1996, 2255,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 101, 2673, 2008,  ...,    0,    0,    0],\n",
            "        [ 101, 2009, 1521,  ...,    0,    0,    0],\n",
            "        [ 101, 2035, 1045,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs[\"input_ids\"].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdPSLBTo5Hli",
        "outputId": "6a154c7b-28b3-4c19-f5b9-acdf879ae34b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 48])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why is it 32*48?"
      ],
      "metadata": {
        "id": "TZ4R3fpvVkTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(max(Q1_sentence_nltk, key=len))\n",
        "print(len(max(Q1_sentence_nltk, key=len).split(\" \")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fT8YZGto0jmS",
        "outputId": "11b836dc-459b-40b5-8ebc-19004b074059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A puff of wind swept across the meadow and through her hair before it slipped into the woods to rustle branches and send back snatches of distant barking-a hazy sound that seemed to reach us from the doorway to another world.\n",
            "41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_sentence = \"A puff of wind swept across the meadow and through her hair before it slipped into the woods to rustle branches and send back snatches of distant barking-a hazy sound that seemed to reach us from the doorway to another world.\"\n",
        "tokens = tokenizer.tokenize(max_sentence)\n",
        "\n",
        "print(tokens)\n",
        "print(len(tokens))\n",
        "max_input = tokenizer(max_sentence)\n",
        "print(max_input)\n",
        "print(len(max_input.input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3dLbRNM4t2k",
        "outputId": "3b13dfc1-2f58-4812-fbf9-3c483cfe43d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'puff', 'of', 'wind', 'swept', 'across', 'the', 'meadow', 'and', 'through', 'her', 'hair', 'before', 'it', 'slipped', 'into', 'the', 'woods', 'to', 'rust', '##le', 'branches', 'and', 'send', 'back', 'snatch', '##es', 'of', 'distant', 'barking', '-', 'a', 'hazy', 'sound', 'that', 'seemed', 'to', 'reach', 'us', 'from', 'the', 'doorway', 'to', 'another', 'world', '.']\n",
            "46\n",
            "{'input_ids': [101, 1037, 23893, 1997, 3612, 7260, 2408, 1996, 13244, 1998, 2083, 2014, 2606, 2077, 2009, 5707, 2046, 1996, 5249, 2000, 18399, 2571, 5628, 1998, 4604, 2067, 23365, 2229, 1997, 6802, 19372, 1011, 1037, 26710, 2614, 2008, 2790, 2000, 3362, 2149, 2013, 1996, 7086, 2000, 2178, 2088, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnRTg6QB7ehJ",
        "outputId": "bf407fe2-2095-4e3c-aab5-48f79f3413d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1037,\n",
              " 23893,\n",
              " 1997,\n",
              " 3612,\n",
              " 7260,\n",
              " 2408,\n",
              " 1996,\n",
              " 13244,\n",
              " 1998,\n",
              " 2083,\n",
              " 2014,\n",
              " 2606,\n",
              " 2077,\n",
              " 2009,\n",
              " 5707,\n",
              " 2046,\n",
              " 1996,\n",
              " 5249,\n",
              " 2000,\n",
              " 18399,\n",
              " 2571,\n",
              " 5628,\n",
              " 1998,\n",
              " 4604,\n",
              " 2067,\n",
              " 23365,\n",
              " 2229,\n",
              " 1997,\n",
              " 6802,\n",
              " 19372,\n",
              " 1011,\n",
              " 1037,\n",
              " 26710,\n",
              " 2614,\n",
              " 2008,\n",
              " 2790,\n",
              " 2000,\n",
              " 3362,\n",
              " 2149,\n",
              " 2013,\n",
              " 1996,\n",
              " 7086,\n",
              " 2000,\n",
              " 2178,\n",
              " 2088,\n",
              " 1012]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(max_input[\"input_ids\"]))\n",
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3CiE9nD7mhJ",
        "outputId": "4ddaccfa-c38b-4b80-dab8-860f6a8a711c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] a puff of wind swept across the meadow and through her hair before it slipped into the woods to rustle branches and send back snatches of distant barking - a hazy sound that seemed to reach us from the doorway to another world. [SEP]\n",
            "a puff of wind swept across the meadow and through her hair before it slipped into the woods to rustle branches and send back snatches of distant barking - a hazy sound that seemed to reach us from the doorway to another world.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModel.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122,
          "referenced_widgets": [
            "a9841f7d2a6748ac8b499df0d990afaa",
            "4bf024c2b50f41728954ffd7e4b2dbd7",
            "88f1797ad823432ca51573f674e8ec25",
            "2e0d0229a3a84b1397a71518b8df9409",
            "0287e508c46c4593b864affc3da4f665",
            "90c27580a9a544b38e23a8be4729b46a",
            "e3d9d8d3c3a8477792c2cc5cafb15a19",
            "ec1c4c465b4c4e7d9d6997f46b6d24fa",
            "d975f6fb88bc44c0b3b9fb0b2d18b40c",
            "a1a6f2d71f5547aeba22671c27cc0f65",
            "d968f72126be4c44b7e84e53ac20a3c9"
          ]
        },
        "id": "WcOBn7cFo6Yw",
        "outputId": "dbf15350-a56d-4598-8fc9-d55b00d2f5f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9841f7d2a6748ac8b499df0d990afaa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**inputs)\n",
        "print(outputs.last_hidden_state.shape)"
      ],
      "metadata": {
        "id": "5ZEZPsdrpAEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)"
      ],
      "metadata": {
        "id": "FIAM-07XpCBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.logits.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6HlB_zHpKDe",
        "outputId": "e9754f91-f186-4ff0-b16f-6b79f15fc527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0wJutLfpM4F",
        "outputId": "ffe74a4f-fd07-4708-bccb-a62e33691020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-2.9097,  3.0603],\n",
            "        [-2.9906,  3.1538],\n",
            "        [-3.0856,  3.3513],\n",
            "        [ 0.5252, -0.3588],\n",
            "        [ 1.7268, -1.3978],\n",
            "        [-0.1638,  0.2964],\n",
            "        [ 2.4347, -1.9709],\n",
            "        [-1.5700,  1.5212],\n",
            "        [ 0.5479, -0.2465],\n",
            "        [-3.0891,  3.2033],\n",
            "        [-3.8900,  4.1634],\n",
            "        [ 4.2343, -3.4772],\n",
            "        [-3.5073,  3.6947],\n",
            "        [ 3.6969, -3.0664],\n",
            "        [-2.7650,  2.8711],\n",
            "        [-3.7879,  4.0219],\n",
            "        [-2.5296,  2.5948],\n",
            "        [-3.1700,  3.3370],\n",
            "        [ 2.3666, -1.9988],\n",
            "        [-3.8532,  4.0928],\n",
            "        [ 4.1809, -3.4629],\n",
            "        [-2.3029,  2.3895],\n",
            "        [-4.0548,  4.2781],\n",
            "        [-1.8823,  1.9305],\n",
            "        [ 3.0080, -2.4859],\n",
            "        [ 2.7569, -2.3052],\n",
            "        [ 3.5068, -2.8443],\n",
            "        [ 3.2776, -2.6787],\n",
            "        [ 3.4121, -2.7691],\n",
            "        [ 2.0810, -1.6311],\n",
            "        [-3.2181,  3.4149],\n",
            "        [ 0.7319, -0.5070]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuHk50fnpuQv",
        "outputId": "342107c6-a1be-4607-fd10-961aed2abeae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.5478e-03, 9.9745e-01],\n",
            "        [2.1409e-03, 9.9786e-01],\n",
            "        [1.5987e-03, 9.9840e-01],\n",
            "        [7.0765e-01, 2.9235e-01],\n",
            "        [9.5789e-01, 4.2105e-02],\n",
            "        [3.8693e-01, 6.1307e-01],\n",
            "        [9.8794e-01, 1.2061e-02],\n",
            "        [4.3471e-02, 9.5653e-01],\n",
            "        [6.8877e-01, 3.1123e-01],\n",
            "        [1.8469e-03, 9.9815e-01],\n",
            "        [3.1792e-04, 9.9968e-01],\n",
            "        [9.9955e-01, 4.4744e-04],\n",
            "        [7.4454e-04, 9.9926e-01],\n",
            "        [9.9885e-01, 1.1541e-03],\n",
            "        [3.5544e-03, 9.9645e-01],\n",
            "        [4.0561e-04, 9.9959e-01],\n",
            "        [5.9148e-03, 9.9409e-01],\n",
            "        [1.4907e-03, 9.9851e-01],\n",
            "        [9.8745e-01, 1.2550e-02],\n",
            "        [3.5395e-04, 9.9965e-01],\n",
            "        [9.9952e-01, 4.7879e-04],\n",
            "        [9.0807e-03, 9.9092e-01],\n",
            "        [2.4041e-04, 9.9976e-01],\n",
            "        [2.1609e-02, 9.7839e-01],\n",
            "        [9.9591e-01, 4.0950e-03],\n",
            "        [9.9371e-01, 6.2927e-03],\n",
            "        [9.9826e-01, 1.7418e-03],\n",
            "        [9.9742e-01, 2.5827e-03],\n",
            "        [9.9794e-01, 2.0636e-03],\n",
            "        [9.7616e-01, 2.3845e-02],\n",
            "        [1.3146e-03, 9.9869e-01],\n",
            "        [7.7537e-01, 2.2463e-01]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.id2label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcbZ30IRpy34",
        "outputId": "51982d38-94b9-4de5-d2be-ead07cb0a983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'NEGATIVE', 1: 'POSITIVE'}"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###How to get the predicted label for each sentence?"
      ],
      "metadata": {
        "id": "Fc_y2-vL9Ziu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m9RHsYcK9d_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TLY9-pD99eGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IwlGOw829eMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QHF7uNe09ePA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UXTVtFdT9eRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L1YXIWth9eUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o5M8rmIM9eWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZHsAnnrl9eZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_labels = torch.argmax(predictions, dim=1)\n",
        "pred_labels"
      ],
      "metadata": {
        "id": "XxiE6IQGwHur",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "300494ac-d53b-4949-a58e-2615344ce91e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
              "        0, 0, 0, 0, 0, 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_labels = predictions.max(1).indices\n",
        "print(pred_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u-GkDF29cvq",
        "outputId": "4f963997-de48-4f83-8907-13ebd3911db1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
            "        0, 0, 0, 0, 0, 0, 1, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to get the most positive and negative sentence in this paragraph?"
      ],
      "metadata": {
        "id": "0X0t59oW9hZg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rd-OHUeH9m7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f_3alL4Y9m-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MflsTe2Y9nA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OGeavN349nDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vSMGMge09nGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ANzo4Fds9nIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MKadhElr9nK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_labels = predictions.max(0).indices\n",
        "print(pred_labels)\n",
        "print(Q1_sentence_nltk[int(pred_labels[0])])\n",
        "print(Q1_sentence_nltk[int(pred_labels[1])])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZkm4O3Y9rSY",
        "outputId": "52fc58ce-c580-4013-ede1-ef1717eb9d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([11, 22])\n",
            "When I was in the scene, I hardly paid it any mind.\n",
            "The smell of the grass, the faint chill of the wind, the line of the hills, the barking of a dog: these are the first things, and they come with absolute clarity.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "most_score = predictions.max(0)\n",
        "most_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wYVND3U_gvI",
        "outputId": "b157bf63-5de4-4700-a434-0a3fd34abf6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.return_types.max(\n",
              "values=tensor([0.9996, 0.9998], grad_fn=<MaxBackward0>),\n",
              "indices=tensor([11, 22]))"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUZY2g3mfAN6"
      },
      "outputs": [],
      "source": [
        "Q1_result = classifier(Q1_sentence_nltk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKfvi2yGfrqO",
        "outputId": "6f625361-8f54-4dd1-dc5a-27b5737cc14c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POSITIVE 0.9974521994590759 Eighteen years have gone by, and still I can bring back every detail of that day in the meadow.\n",
            "POSITIVE 0.9978590607643127 Washed clean of summer’s dust by days of gentle rain, the mountains wore a deep, brilliant green.\n",
            "POSITIVE 0.9984012246131897 The October breeze set white fronds of head-tall grasses swaying.\n",
            "NEGATIVE 0.7076473832130432 One long streak of cloud hung pasted across a dome of frozen blue.\n",
            "NEGATIVE 0.957894504070282 It almost hurt to look at that faroff sky.\n",
            "POSITIVE 0.6130686402320862 A puff of wind swept across the meadow and through her hair before it slipped into the woods to rustle branches and send back snatches of distant barking-a hazy sound that seemed to reach us from the doorway to another world.\n",
            "NEGATIVE 0.9879387617111206 We heard no other sounds.\n",
            "POSITIVE 0.9565290808677673 We met no other people.\n",
            "NEGATIVE 0.6887699961662292 We saw only two bright, red birds leap startled from the center of the meadow and dart into the woods.\n",
            "POSITIVE 0.9981531500816345 As we ambled along, Naoko spoke to me of wells.\n",
            "POSITIVE 0.9996820688247681 Memory is a funny thing.\n",
            "NEGATIVE 0.9995525479316711 When I was in the scene, I hardly paid it any mind.\n",
            "POSITIVE 0.9992554783821106 I never stopped to think of it as something that would make a lasting impression, certainly never imagined that eighteen years later I would recall it in such detail.\n",
            "NEGATIVE 0.9988459348678589 I didn’t give a damn about the scenery that day.\n",
            "POSITIVE 0.9964455962181091 I was thinking about myself.\n",
            "POSITIVE 0.9995943903923035 I was thinking about the beautiful girl walking next to me.\n",
            "POSITIVE 0.9940851926803589 I was thinking about the two of us together, and then about myself again.\n",
            "POSITIVE 0.9985093474388123 It was the age, that time of life when every sight, every feeling, every thought came back, like a boomerang, to me.\n",
            "NEGATIVE 0.987450122833252 And worse, I was in love.\n",
            "POSITIVE 0.9996460676193237 Love with complications.\n",
            "NEGATIVE 0.9995212554931641 Scenery was the last thing on my mind.\n",
            "POSITIVE 0.9909192323684692 Now, though, that meadow scene is the first thing that comes back to me.\n",
            "POSITIVE 0.9997596144676208 The smell of the grass, the faint chill of the wind, the line of the hills, the barking of a dog: these are the first things, and they come with absolute clarity.\n",
            "POSITIVE 0.9783908724784851 I feel as if I can reach out and trace them with a fingertip.\n",
            "NEGATIVE 0.9959050416946411 And yet, as clear as the scene may be, no one is in it.\n",
            "NEGATIVE 0.9937073588371277 No one.\n",
            "NEGATIVE 0.9982581734657288 Naoko is not there, and neither am I.\n",
            "NEGATIVE 0.9974173307418823 Where could we have disappeared to?\n",
            "NEGATIVE 0.9979363679885864 How could such a thing have happened?\n",
            "NEGATIVE 0.976155161857605 Everything that seemed so important back then-Naoko, and the self I was then, and the world I had then: where could they have all gone?\n",
            "POSITIVE 0.9986854195594788 It’s true, I can’t even bring back Naoko’s face-not right away, at least.\n",
            "NEGATIVE 0.7753749489784241 All I’m left holding is a background, sheer scenery, with no people up front.\n",
            "positive_most_sent:  The smell of the grass, the faint chill of the wind, the line of the hills, the barking of a dog: these are the first things, and they come with absolute clarity.\n",
            "positive score:  0.9997596144676208\n",
            "negative_most_sent:  When I was in the scene, I hardly paid it any mind.\n",
            "negative score:  0.9995525479316711\n"
          ]
        }
      ],
      "source": [
        "positive_most_score = 0\n",
        "positive_most_sent = ''\n",
        "negative_most_score = 0\n",
        "negative_most_sent = ''\n",
        "for i in range(len(Q1_sentence_nltk)):\n",
        "  current_result = Q1_result[i]\n",
        "  current_label = current_result[\"label\"]\n",
        "  current_score = current_result[\"score\"]\n",
        "  current_sentence = Q1_sentence_nltk[i]\n",
        "  print(current_label,current_score,current_sentence)\n",
        "  if current_label == \"POSITIVE\":\n",
        "    if current_score >  positive_most_score:\n",
        "      positive_most_sent = current_sentence\n",
        "      positive_most_score = current_score\n",
        "  if current_label == \"NEGATIVE\":\n",
        "    if current_score >  negative_most_score:\n",
        "      negative_most_sent = current_sentence\n",
        "      negative_most_score = current_score \n",
        "print(\"positive_most_sent: \", positive_most_sent) \n",
        "print(\"positive score: \", positive_most_score)\n",
        "print(\"negative_most_sent: \", negative_most_sent)\n",
        "print(\"negative score: \", negative_most_score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Finish the same task as **above**, instead of using the tokenizer directly, do tokenize, convert tokens into token_ids, add start and end token, do padding and generate attention mask by your own."
      ],
      "metadata": {
        "id": "vEZfQ220Cg11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What should we do to process a single sentence?"
      ],
      "metadata": {
        "id": "ajZifxDBPLzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_sent1 = tokenizer.tokenize(Q1_sentence_nltk[0])\n",
        "  # output = tokenizer(sent)\n",
        "\n",
        "print(tokens_sent1)\n",
        "print(len(tokens_sent1))\n",
        "# print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13882506-35da-4ce5-c4c1-16cf07f6118d",
        "id": "qryEZEqKFann"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['eighteen', 'years', 'have', 'gone', 'by', ',', 'and', 'still', 'i', 'can', 'bring', 'back', 'every', 'detail', 'of', 'that', 'day', 'in', 'the', 'meadow', '.']\n",
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_sent1 = tokenizer.convert_tokens_to_ids(tokens_sent1)\n",
        "ids_sent1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9al5CIOFano",
        "outputId": "4b4fad19-b88d-4952-b90e-df6c4e6e1268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7763,\n",
              " 2086,\n",
              " 2031,\n",
              " 2908,\n",
              " 2011,\n",
              " 1010,\n",
              " 1998,\n",
              " 2145,\n",
              " 1045,\n",
              " 2064,\n",
              " 3288,\n",
              " 2067,\n",
              " 2296,\n",
              " 6987,\n",
              " 1997,\n",
              " 2008,\n",
              " 2154,\n",
              " 1999,\n",
              " 1996,\n",
              " 13244,\n",
              " 1012]"
            ]
          },
          "metadata": {},
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.cls_token_id)\n",
        "print(tokenizer.sep_token_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lR6SQMV8_GUu",
        "outputId": "e3563c2e-bb90-4977-c8b6-13c4d1a0f338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101\n",
            "102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "editted_token_ids_sent1 = torch.tensor([tokenizer.cls_token_id] + ids_sent1 + [tokenizer.sep_token_id])\n",
        "editted_token_ids_sent1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncDyWY_pPj7j",
        "outputId": "d568ef9c-f6d1-4131-990b-5c8d26463fb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  101,  7763,  2086,  2031,  2908,  2011,  1010,  1998,  2145,  1045,\n",
              "         2064,  3288,  2067,  2296,  6987,  1997,  2008,  2154,  1999,  1996,\n",
              "        13244,  1012,   102])"
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(editted_token_ids_sent1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "QILGc3RIPrd4",
        "outputId": "0f90a125-784b-40b4-e812-0a8876fd9758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-218-0606778e3549>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meditted_token_ids_sent1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m         )\n\u001b[1;32m    762\u001b[0m         \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistilbert_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (bs, seq_len, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m         return self.transformer(\n\u001b[1;32m    574\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    120\u001b[0m         embeddings)\n\u001b[1;32m    121\u001b[0m         \"\"\"\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Setting the position-ids to the registered buffer in constructor, it helps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem is that we sent a single sequence to the model, whereas 🤗 Transformers models expect multiple sentences by default. "
      ],
      "metadata": {
        "id": "BAIdfJhhXB_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "editted_multi_d_token_ids_sent1 = torch.tensor([[tokenizer.cls_token_id] + ids_sent1 + [tokenizer.sep_token_id]])\n",
        "model(editted_multi_d_token_ids_sent1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnTQe-zWP4Uf",
        "outputId": "e800af55-76bb-47b8-f9d1-c2b1773e4ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=None, logits=tensor([[-2.9097,  3.0603]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What about processing multiple sentences at one time?"
      ],
      "metadata": {
        "id": "NrodiNZLQm4u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_sent2 = tokenizer.tokenize(Q1_sentence_nltk[1])\n",
        "  # output = tokenizer(sent)\n",
        "\n",
        "print(tokens_sent2)\n",
        "print(len(tokens_sent2))\n",
        "# print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13807917-1a96-486b-cc2e-0651ca8f4ac6",
        "id": "oz_JQpJRRrmi"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['washed', 'clean', 'of', 'summer', '’', 's', 'dust', 'by', 'days', 'of', 'gentle', 'rain', ',', 'the', 'mountains', 'wore', 'a', 'deep', ',', 'brilliant', 'green', '.']\n",
            "22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_sent2 = tokenizer.convert_tokens_to_ids(tokens_sent2)\n",
        "ids_sent2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7452848-8ab2-4e07-d5d1-085c0cd16b98",
        "id": "dfas4NWJRrmo"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8871,\n",
              " 4550,\n",
              " 1997,\n",
              " 2621,\n",
              " 1521,\n",
              " 1055,\n",
              " 6497,\n",
              " 2011,\n",
              " 2420,\n",
              " 1997,\n",
              " 7132,\n",
              " 4542,\n",
              " 1010,\n",
              " 1996,\n",
              " 4020,\n",
              " 5078,\n",
              " 1037,\n",
              " 2784,\n",
              " 1010,\n",
              " 8235,\n",
              " 2665,\n",
              " 1012]"
            ]
          },
          "metadata": {},
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.cls_token_id)\n",
        "print(tokenizer.sep_token_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96602558-e75c-4caf-b3f3-56c788a2d97e",
        "id": "r8VHgUccRrmo"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "101\n",
            "102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "editted_multi_d_token_ids_sent2 = torch.tensor([[tokenizer.cls_token_id] + ids_sent2 + [tokenizer.sep_token_id]])\n",
        "editted_multi_d_token_ids_sent2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63b0d38c-d29b-449b-e809-156f66101ebe",
        "id": "ebsAJa-cRrmo"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 101, 8871, 4550, 1997, 2621, 1521, 1055, 6497, 2011, 2420, 1997, 7132,\n",
              "         4542, 1010, 1996, 4020, 5078, 1037, 2784, 1010, 8235, 2665, 1012,  102]])"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(editted_multi_d_token_ids_sent1.shape)\n",
        "print(editted_multi_d_token_ids_sent2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXI6AD96R5r5",
        "outputId": "a4bd76eb-9c5f-42c9-e472-8b9fbd20a14f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 23])\n",
            "torch.Size([1, 24])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding to generate rectangular shape!!!"
      ],
      "metadata": {
        "id": "Rkis-L2rSK1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to work around this, we’ll use padding to make our tensors have a rectangular shape. Padding makes sure all our sentences have the same length by adding a special word called the padding token to the sentences with fewer values. For example, if you have 10 sentences with 10 words and 1 sentence with 20 words, padding will ensure all the sentences have 20 words.\n",
        "\n",
        "\n",
        "The padding token ID can be found in tokenizer.pad_token_id. \n",
        "\n",
        "\n",
        "In our example, the resulting tensor looks like this:"
      ],
      "metadata": {
        "id": "IN72YyHuSanK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1G4Wd2GTH-e",
        "outputId": "81d4a23c-c963-4252-de60-c2268ffc04e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 225
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_editted_multi_d_token_ids_sent1 = torch.tensor([[tokenizer.cls_token_id] + ids_sent1 + [tokenizer.sep_token_id] + [tokenizer.pad_token_id]])\n",
        "padded_editted_multi_d_token_ids_sent1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jKR-ciPSKd-",
        "outputId": "4b00e29a-dd98-4bc8-d66f-33cfdb477ecc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  101,  7763,  2086,  2031,  2908,  2011,  1010,  1998,  2145,  1045,\n",
              "          2064,  3288,  2067,  2296,  6987,  1997,  2008,  2154,  1999,  1996,\n",
              "         13244,  1012,   102,     0]])"
            ]
          },
          "metadata": {},
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "padded_inputs_ids = torch.stack((padded_editted_multi_d_token_ids_sent1.squeeze(),editted_multi_d_token_ids_sent2.squeeze()))\n",
        "padded_inputs_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6xUmzigSUMY",
        "outputId": "85637f7e-ccc8-41f1-cddb-4e97778bd6f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  101,  7763,  2086,  2031,  2908,  2011,  1010,  1998,  2145,  1045,\n",
              "          2064,  3288,  2067,  2296,  6987,  1997,  2008,  2154,  1999,  1996,\n",
              "         13244,  1012,   102,     0],\n",
              "        [  101,  8871,  4550,  1997,  2621,  1521,  1055,  6497,  2011,  2420,\n",
              "          1997,  7132,  4542,  1010,  1996,  4020,  5078,  1037,  2784,  1010,\n",
              "          8235,  2665,  1012,   102]])"
            ]
          },
          "metadata": {},
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(editted_multi_d_token_ids_sent1).logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aF7OIeSSUW4",
        "outputId": "d9a87fd5-7a47-455d-8cfb-3dede9fe8f0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.9097,  3.0603]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(editted_multi_d_token_ids_sent2).logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yGGDytpUGyx",
        "outputId": "d610f442-1516-4002-80d9-80db0068bf6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.9906,  3.1538]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(padded_inputs_ids).logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUzQZ3hNUJdf",
        "outputId": "f9210d2c-772e-41c1-cfb2-ca8b66c8d7fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.7389,  2.8820],\n",
              "        [-2.9906,  3.1538]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There’s something wrong with the logits in our batched predictions: the first row should be the same as the logits for the first sentence, but we’ve got completely different values!\n",
        "\n",
        "This is because the key feature of Transformer models is attention layers that contextualize each token. These will take into account the padding tokens since they attend to all of the tokens of a sequence. To get the same result when passing individual sentences of different lengths through the model or when passing a batch with the same sentences and padding applied, we need to tell those attention layers to ignore the padding tokens. This is done by using an attention mask."
      ],
      "metadata": {
        "id": "tpiov9WYUOx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate attention mask!"
      ],
      "metadata": {
        "id": "6J27_kIDUm07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attention_mask_sent1_2 = torch.ones(padded_inputs_ids.shape)\n",
        "attention_mask_sent1_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7v-0A7y-UWWg",
        "outputId": "c6078713-6fc1-49b0-e634-a1e9b82b8e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_mask_sent1_2[0,-1] = 0\n",
        "attention_mask_sent1_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTE8k27nUyot",
        "outputId": "d03ea5ba-6f7e-4d52-928a-cc0fffb1b727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "         1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(torch.tensor(padded_inputs_ids), attention_mask=torch.tensor(attention_mask_sent1_2)).logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLETHiMXU3jr",
        "outputId": "6ef80dac-4e36-479b-c936-de34276265f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.9097,  3.0603],\n",
              "        [-2.9906,  3.1538]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now try to process the whole paragraph at once!"
      ],
      "metadata": {
        "id": "Zo5VuW9bVOZW"
      }
    }
  ]
}